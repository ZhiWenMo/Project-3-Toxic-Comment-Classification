{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8)\n",
      "(153164, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./Data/train.csv')\n",
    "train_de_df = pd.read_csv('./Data/train_de.csv')\n",
    "train_es_df = pd.read_csv('./Data/train_es.csv')\n",
    "train_fr_df = pd.read_csv('./Data/train_fr.csv')\n",
    "test_df = pd.read_csv('./Data/test.csv')\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTTEXT_WIKI_EMBEDFILE = './fasttext_wordvec/wiki.en.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTTEXT_CRAWL_EMBEDFILE = './fasttext_wordvec/ycrawl-300d-2M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_EMBEDFILE = './fasttext_wordvec/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['comment_text'].values\n",
    "X_train_de = train_de_df['comment_text'].values\n",
    "X_train_es = train_es_df['comment_text'].values\n",
    "X_train_fr = train_fr_df['comment_text'].values\n",
    "label_column_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train_df[label_column_names].values\n",
    "X_test = test_df['comment_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df\n",
    "del train_de_df\n",
    "del train_es_df\n",
    "del train_fr_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text,sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 60000\n",
    "max_len = 150\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train_all = np.concatenate([X_train,X_train_de,X_train_es,X_train_fr])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def text2sequence(vocab_size,max_len,X_train,X_test):\n",
    "    tokenizer = text.Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(list(X_train)+list(X_test))\n",
    "    word_index = tokenizer.word_index\n",
    "    return tokenizer,word_index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenizer, word_index = text2sequence(vocab_size,max_len,X_train_all,X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del X_train_all"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_test = sequence.pad_sequences(tokenizer.texts_to_sequences(X_test),maxlen=max_len)\n",
    "X_test.dump('F:/X_test.npy')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train = sequence.pad_sequences(tokenizer.texts_to_sequences(X_train),maxlen=max_len)\n",
    "X_train.dump('./Data/X_train.npy')\n",
    "del X_train\n",
    "X_train_de = sequence.pad_sequences(tokenizer.texts_to_sequences(X_train_de),maxlen=max_len)\n",
    "X_train_de.dump('./Data/X_train_de.npy')\n",
    "del X_train_de\n",
    "X_train_es = sequence.pad_sequences(tokenizer.texts_to_sequences(X_train_es),maxlen=max_len)\n",
    "\n",
    "X_train_es.dump('F:/X_train_es.npy')\n",
    "del X_train_es\n",
    "X_train_fr = sequence.pad_sequences(tokenizer.texts_to_sequences(X_train_fr),maxlen=max_len)\n",
    "X_train_fr.dump('F:/X_train_fr.npy')\n",
    "del X_train_fr\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def gen_embedding_dict(fliepath,skip=True):\n",
    "    with open(fliepath,'r',encoding='utf-8') as f:\n",
    "        wordvec = {}\n",
    "        if skip:\n",
    "            vs, ems = f.readline()[:-1].split()\n",
    "            print('In this embedding file,\\n vocab size: %s,\\n embed size: %s' % (vs,ems))\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            key_v_list = line[:-1].rstrip().rsplit(' ',300)\n",
    "            if len(key_v_list[1:]) != 300:\n",
    "                raise ValueError('embed size is not 300:',key_v_list)\n",
    "            try:\n",
    "                wordvec[key_v_list[0]] = np.asarray(key_v_list[1:],dtype='float32')\n",
    "            except:\n",
    "                raise ValueError(key_v_list)\n",
    "            line = f.readline()\n",
    "    return wordvec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fasttext_wiki_dict = gen_embedding_dict(FASTTEXT_WIKI_EMBEDFILE)\n",
    "fasttext_crawl_dict = gen_embedding_dict(FASTTEXT_CRAWL_EMBEDFILE)\n",
    "glove_crawl_dict = gen_embedding_dict(GLOVE_EMBEDFILE,skip=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vocab_size = min(vocab_size,len(word_index))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def gen_embedding_mat(embe_dict):\n",
    "    embedding_matrix = np.random.uniform(-1,1,(vocab_size+1,embed_size))\n",
    "    embedding_matrix[0] = np.zeros((1,embed_size))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i > vocab_size:break\n",
    "        if word in embe_dict:\n",
    "            embedding_matrix[i] = embe_dict[word]\n",
    "        else:\n",
    "            continue\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fasttext_wiki_mat = gen_embedding_mat(fasttext_wiki_dict)\n",
    "fasttext_crawl_mat = gen_embedding_mat(fasttext_crawl_dict)\n",
    "glove_crawl_mat = gen_embedding_mat(glove_crawl_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del fasttext_wiki_dict\n",
    "\n",
    "del fasttext_crawl_dict\n",
    "\n",
    "del glove_crawl_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftwiki_mat_file = 'F:/ftwiki.npy'\n",
    "#fasttext_wiki_mat.dump(ftwiki_mat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftwiki_mat = np.load(ftwiki_mat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftcrawl_mat_file = 'F:/ftcrawl.npy'\n",
    "#fasttext_crawl_mat.dump(ftcrawl_mat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftcrawl_mat = np.load(ftcrawl_mat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_mat_file = 'F:/glove_mat.npy'\n",
    "#glove_crawl_mat.dump(glove_mat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_matrix = np.load(glove_mat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('./Data/X_train.npy')\n",
    "X_train_de = np.load('./Data/X_train_de.npy')\n",
    "X_train_es = np.load('F:/X_train_es.npy')\n",
    "X_train_fr = np.load('F:/X_train_fr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(splits,X_train,y_train,*args):\n",
    "    kf = KFold(splits)\n",
    "    for train_idx, valid_idx in kf.split(X_train):\n",
    "        train_x = np.concatenate([x[train_idx] for x in args],axis=0)\n",
    "        train_x = np.concatenate([X_train[train_idx],train_x],axis=0)\n",
    "        train_y = np.concatenate([y_train[train_idx] for _ in args],axis=0)\n",
    "        train_y = np.concatenate([y_train[train_idx],train_y],axis=0)\n",
    "        \n",
    "        valid_x = np.concatenate([x[valid_idx] for x in args],axis=0)\n",
    "        valid_x = np.concatenate([X_train[valid_idx],valid_x],axis=0)\n",
    "        valid_y = np.concatenate([y_train[valid_idx] for _ in args],axis=0)\n",
    "        valid_y = np.concatenate([y_train[valid_idx],valid_y],axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        yield train_x, train_y, valid_x, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, average\n",
    "from keras.layers import GRU ,CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras.initializers import Constant\n",
    "from keras import initializers,regularizers,constraints\n",
    "from keras.callbacks import Callback,EarlyStopping,ModelCheckpoint\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_norm():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allocator_type = 'BFC'\n",
    "    with tf.Session(config = config) as s:\n",
    "        wiki_weights = Constant(ftwiki_mat)\n",
    "\n",
    "        inputs = Input(shape=(max_len,))\n",
    "\n",
    "        embed_layer1 = Embedding(vocab_size+1,embed_size,embeddings_initializer=wiki_weights)\n",
    "        embed_layer1.trainable = False\n",
    "\n",
    "        embed_out1 = embed_layer1(inputs)\n",
    "\n",
    "\n",
    "        s_dout = SpatialDropout1D(0.2)(embed_out1)\n",
    "\n",
    "        bi_rnn = Bidirectional(CuDNNGRU(80,return_sequences=True))(s_dout)\n",
    "\n",
    "        avg_pool = GlobalAveragePooling1D()(bi_rnn)\n",
    "        max_pool = GlobalMaxPooling1D()(bi_rnn)\n",
    "        concat = concatenate([avg_pool,max_pool])\n",
    "\n",
    "        outputs = Dense(6,activation='sigmoid')(concat)\n",
    "\n",
    "        opt = adam(lr=0.001)\n",
    "\n",
    "        model = Model(inputs=[inputs],outputs=outputs)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                     optimizer=opt,\n",
    "                     metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 300)     18000300    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 150, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 150, 160)     183360      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 160)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 160)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 320)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            1926        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 18,185,586\n",
      "Trainable params: 185,286\n",
      "Non-trainable params: 18,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use simple avg different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 574452 samples, validate on 63832 samples\n",
      "Epoch 1/5\n",
      "574452/574452 [==============================] - 347s 604us/step - loss: 0.0511 - acc: 0.9814 - val_loss: 0.0456 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98293, saving model to weights_wiki_best.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.984765 \n",
      "\n",
      "Epoch 2/5\n",
      "574452/574452 [==============================] - 334s 582us/step - loss: 0.0428 - acc: 0.9838 - val_loss: 0.0446 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98293 to 0.98335, saving model to weights_wiki_best.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.985631 \n",
      "\n",
      "Epoch 3/5\n",
      "574452/574452 [==============================] - 336s 585us/step - loss: 0.0398 - acc: 0.9848 - val_loss: 0.0452 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.98335\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.985750 \n",
      "\n",
      "Epoch 4/5\n",
      "574452/574452 [==============================] - 336s 585us/step - loss: 0.0375 - acc: 0.9857 - val_loss: 0.0458 - val_acc: 0.9825\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.98335\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.985322 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(*args,train_size=0.95):\n",
    "    X_tra, X_val, X_tra1, X_val1,X_tra2, X_val2, X_tra3, X_val3, y_tra, y_val = train_test_split(*args,train_size=train_size,random_state=42)\n",
    "    X_tra_aug = np.concatenate([X_tra,X_tra1,X_tra2,X_tra3],axis=0)\n",
    "    X_val_aug = np.concatenate([X_val,X_val1,X_val2,X_val3],axis=0)\n",
    "    \n",
    "    y_tra_aug = np.concatenate([y_tra for _ in range(len(args)-1)],axis=0)\n",
    "    y_val_agu = np.concatenate([y_val for _ in range(len(args)-1)],axis=0)\n",
    "    return X_tra_aug,X_val_aug,y_tra_aug,y_val_agu\n",
    "\n",
    "X_tra,X_val,y_tra,y_val = load_data(X_train,X_train_de,X_train_es,X_train_fr,y_train,train_size=0.9)\n",
    "\n",
    "filepath=\"weights_wiki_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=2)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "callback_list = [checkpoint,early,RocAuc]\n",
    "\n",
    "model = None\n",
    "model = get_model_norm()\n",
    "\n",
    "history = model.fit(X_tra,y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                callbacks=callback_list,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('F:/X_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use CV"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_model_cv():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allocator_type = 'BFC'\n",
    "    with tf.Session(config = config) as s:\n",
    "        wiki_weights = Constant(ftwiki_mat)\n",
    "\n",
    "        crawl_weights = Constant(ftcrawl_mat)\n",
    "\n",
    "        g_crawl_weights = Constant(glove_matrix)\n",
    "\n",
    "        inputs = Input(shape=(max_len,))\n",
    "\n",
    "        embed_layer1 = Embedding(vocab_size+1,embed_size,embeddings_initializer=wiki_weights)\n",
    "        embed_layer1.trainable = False\n",
    "\n",
    "        embed_layer2 = Embedding(vocab_size+1,embed_size,embeddings_initializer=crawl_weights)\n",
    "        embed_layer2.trainable = False\n",
    "\n",
    "        embed_layer3 = Embedding(vocab_size+1,embed_size,embeddings_initializer=g_crawl_weights)\n",
    "        embed_layer3.trainable = False\n",
    "\n",
    "        embed_out1 = embed_layer1(inputs)\n",
    "        embed_out2 = embed_layer2(inputs)\n",
    "        embed_out3 = embed_layer3(inputs)\n",
    "\n",
    "        concat_embed = concatenate([embed_out1,embed_out2,embed_out3])\n",
    "\n",
    "        s_dout = SpatialDropout1D(0.2)(concat_embed)\n",
    "\n",
    "        bi_rnn = Bidirectional(CuDNNGRU(80,return_sequences=True))(s_dout)\n",
    "\n",
    "        avg_pool = GlobalAveragePooling1D()(bi_rnn)\n",
    "        max_pool = GlobalMaxPooling1D()(bi_rnn)\n",
    "        concat = concatenate([avg_pool,max_pool])\n",
    "\n",
    "        outputs = Dense(6,activation='sigmoid')(concat)\n",
    "\n",
    "        opt = adam(lr=0.001)\n",
    "\n",
    "        model = Model(inputs=[inputs],outputs=outputs)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                     optimizer=opt,\n",
    "                     metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predictions = np.zeros((X_test.shape[0],6))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "folds = 8"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i, (train_x, train_y, valid_x, valid_y) in enumerate(generate(folds,X_train,y_train,X_train_de,X_train_es,X_train_fr)):\n",
    "    print('\\nFold',i)\n",
    "    \n",
    "    filepath=\"weights_base.\"+str(i)+\"best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=2)\n",
    "    RocAuc = RocAucEvaluation(validation_data=(valid_x, valid_y), interval=1)\n",
    "    callback_list = [checkpoint,early,RocAuc]\n",
    "    \n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    \n",
    "    model = None\n",
    "    model = get_model_cv()\n",
    "    \n",
    "    history = model.fit(train_x,train_y, batch_size=batch_size, epochs=epochs, validation_data=(valid_x, valid_y),\n",
    "                    callbacks=callback_list,shuffle=True)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    model.load_weights(filepath)\n",
    "    \n",
    "    predictions += model.predict(X_test,batch_size=64) / folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_df = pd.read_csv('./Data/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the value in lable -1 not use for scoring\n",
    "count_indices = test_labels_df.loc[test_labels_df['toxic']>=0].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_labels_df[label_column_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9821473815187113"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_labels[count_indices],predictions[count_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./Data/sample_submission.csv')\n",
    "submission[label_column_names] = predictions\n",
    "submission.to_csv('submission_wiki.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
